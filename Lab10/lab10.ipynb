{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: 25/12 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: 06/01 ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see it this way: picking 3 numbers whose sum is 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1449,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice, random, randint\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(sum(c)==15 for c in combinations({1,2,3,4,5,6},3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "metadata": {},
   "outputs": [],
   "source": [
    "State= namedtuple('State',['x','o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC=[2,7,6,\n",
    "       9,5,1,\n",
    "       4,3,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\" Checks if elements is winning\"\"\"\n",
    "    return any(sum(c)==15 for c in combinations(elements,3))\n",
    "\n",
    "def state_value(pos:State):\n",
    "    \"\"\"Evaluate position: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\" Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            index=r*3+c\n",
    "            if MAGIC[index] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[index] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('-', end='')\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1455,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self._symbol=symbol\n",
    "        \n",
    "    @property\n",
    "    def symbol(self)-> int:\n",
    "        return self._symbol\n",
    "    \n",
    "    def move(self,available_moves, state=None):\n",
    "        #print(\"random move\")\n",
    "        return choice(list(available_moves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1456,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, symbol):\n",
    "        self.q_table = {}\n",
    "        self.symbol=symbol\n",
    "        self._winning_games=0\n",
    "        self._drawn_games=0\n",
    "        self.exploration_rate=0.5\n",
    "        self.alpha=0.7\n",
    "        self.gamma=0.9\n",
    "        self.is_train=True\n",
    "\n",
    "\n",
    "    def move(self, available_moves, state)-> int:\n",
    "        \n",
    "        if self.exploration_rate>0.01:\n",
    "            self.exploration_rate*=0.99\n",
    "\n",
    "        if self.alpha>0.1:\n",
    "            self.alpha*=0.99\n",
    "\n",
    "        if self.gamma<0.99:\n",
    "            self.gamma*=1.01\n",
    "\n",
    "        state_key = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        if random() < self.exploration_rate:\n",
    "            # Scelta casuale di un'azione\n",
    "            action = choice(list(available_moves))\n",
    "        else:\n",
    "            # Scelta dell'azione basata sulla Q-table\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys(range(1,10), 0)\n",
    "                \n",
    "            action = max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.update_q_table(action, available_moves, state)\n",
    "       \n",
    "        if action not in available_moves:\n",
    "            action=choice(list(available_moves))\n",
    "            #self.update_q_table(action, available_moves, state)\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys([action], 0)\n",
    "            return action\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def add_winning(self):\n",
    "        self._winning_games+=1\n",
    "        \n",
    "    def add_drawn(self):\n",
    "        self._drawn_games+=1\n",
    "\n",
    "    def update_q_table(self, action, available_moves, state, reward=None):\n",
    "        #print(\"updating\")\n",
    "        state_key=(frozenset(state.x), frozenset(state.o))\n",
    "        if reward is None:\n",
    "\n",
    "            if action not in available_moves:\n",
    "                reward=-5\n",
    "                \n",
    "            else:\n",
    "                reward=0.1\n",
    "                \n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = dict.fromkeys([action], 0)\n",
    "        \n",
    "        temp_state=deepcopy(state)\n",
    "        temp_state.x.add(action)\n",
    "\n",
    "        new_state = deepcopy(state)\n",
    "        new_state.x.add(action)\n",
    "        next_state_key = (frozenset(new_state.x), frozenset(new_state.o))\n",
    "\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = dict.fromkeys(range(1,10), 0)\n",
    "\n",
    "        self.q_table[state_key][action] = (1 - self.alpha) * self.q_table[state_key].get(action, 0) + self.alpha * (reward + self.gamma * (max(self.q_table[next_state_key].values(), default=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def game(p1,p2, index):\n",
    "    \n",
    "    trajectory=list()\n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "    \n",
    "    players=[p1,p2]\n",
    "\n",
    "    while True:\n",
    "    \n",
    "        current_player=players[index]\n",
    "\n",
    "        move=current_player.move(list(available),state)\n",
    "        trajectory.append((deepcopy(state), move))\n",
    "        available.remove(move)\n",
    "        \n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            if(win(state.o)) or not available:\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            if(win(state.x)) or not available:\n",
    "                break\n",
    "        last_index=index\n",
    "        index=1-index\n",
    "\n",
    "    # i compute the final reward\n",
    "        \n",
    "    final_reward=state_value(state)\n",
    "    #print(index, final_reward)\n",
    "    return trajectory, final_reward, last_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o\n",
    "p1=RandomPlayer(-1)\n",
    "#x\n",
    "p2=QLearningAgent(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1459,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 650/10000 [00:00<00:04, 2187.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2624.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q AGENT STARTS\n",
      "Winning percentage of the agent  80.08\n",
      "Drawn percentage of the agent  14.000000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2820.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM STARTS\n",
      "Winning percentage of the agent  48.59\n",
      "Drawn percentage of the agent  38.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(p1,p2, index):\n",
    "\n",
    "\n",
    "      num_iterations=10_000\n",
    "      p2._winning_games=0\n",
    "      p2._drawn_games=0\n",
    "\n",
    "      for _ in tqdm(range(num_iterations)):\n",
    "\n",
    "            trajectory, final_reward=game(p1,p2,index)\n",
    "\n",
    "            if final_reward ==0:\n",
    "                  p2.add_drawn()\n",
    "            elif final_reward==1:\n",
    "                  p2.add_winning()\n",
    "\n",
    "            if final_reward == -1:\n",
    "            \n",
    "                  s=trajectory[-2][0]\n",
    "                  a=trajectory[-2][1]\n",
    "\n",
    "            else:  \n",
    "                  s=trajectory[-1][0]\n",
    "                  a=trajectory[-1][1]   \n",
    "                  \n",
    "\n",
    "            p2.update_q_table(a,(),s,final_reward)\n",
    "            \n",
    "\n",
    "      if index==0:\n",
    "            print(\"RANDOM STARTS\")\n",
    "      else:\n",
    "            print(\"Q AGENT STARTS\")\n",
    "      print(\"Winning percentage of the agent \",p2._winning_games/num_iterations*100)\n",
    "      print(\"Drawn percentage of the agent \",p2._drawn_games/num_iterations*100)\n",
    "      #for c,v in p2.q_table.items():\n",
    "            #print(c,v)\n",
    "      \n",
    "train(p1,p2,1)\n",
    "train(p1,p2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 62806.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM STARTS\n",
      "winning or tie 0.904\n",
      "losing  0.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 26824.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q AGENT STARTS\n",
      "winning or tie 0.958\n",
      "losing  0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test(p1,p2, index):\n",
    "    \n",
    "    global count_winning\n",
    "    global count_losing\n",
    "    global count_tie\n",
    "\n",
    "    #trajectory=list()\n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "    \n",
    "    players=[p1,p2]\n",
    "    \n",
    "\n",
    "    while True:\n",
    "    \n",
    "        current_player=players[index]\n",
    "\n",
    "        move=current_player.move(list(available),state)\n",
    "        #trajectory.append((deepcopy(state), move))\n",
    "        \n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            available.remove(move) \n",
    "            if(win(state.o)) or not available:\n",
    "                if (win(state.o)):\n",
    "                    #print(\"O \")\n",
    "                    \n",
    "                    count_losing+=1\n",
    "                else: \n",
    "                    #print(\"tie\")\n",
    "                    count_tie+=1\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            #trajectory.append((deepcopy(state),move))\n",
    "            available.remove(move) \n",
    "            if(win(state.x)) or not available:\n",
    "                #current_player.add_winning()\n",
    "                if(win(state.x)):\n",
    "                    #print(\"X\")\n",
    "                    count_winning+=1\n",
    "                else:\n",
    "                    #print(\"tie\")\n",
    "                    count_tie+=1\n",
    "\n",
    "                break\n",
    "                \n",
    "        index=1-index\n",
    "    return\n",
    "\n",
    "\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "\n",
    "p2.is_train=False\n",
    "\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    test(p1,p2,0)\n",
    "\n",
    "print(\"RANDOM STARTS\")\n",
    "    \n",
    "print(\"winning or tie\", (count_winning+count_tie)/1000)\n",
    "print(\"losing \", count_losing/1000)\n",
    "\n",
    "\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    test(p1,p2,1)\n",
    "print(\"Q AGENT STARTS\")\n",
    "\n",
    "print(\"winning or tie\", (count_winning+count_tie)/1000)\n",
    "print(\"losing \", count_losing/1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for c,v in p2.q_table.items():\n",
    "    print(c,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTECARLO (attempts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1462,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self._symbol=symbol\n",
    "        self._dict_moves=defaultdict(float)\n",
    "        self._winning_games=0\n",
    "\n",
    "    @property\n",
    "    def symbol(self)-> int:\n",
    "        return self._symbol\n",
    "    \n",
    "    @property\n",
    "    def dict_moves(self)-> int:\n",
    "        return self._dict_moves\n",
    "    \n",
    "    def move(self,available_moves, state)->int:\n",
    "\n",
    "\n",
    "        temp_state = deepcopy(state) \n",
    "        if random()<0.1:\n",
    "            best_move = choice(list(available_moves))\n",
    "            temp_state.x.add(best_move)\n",
    "            if (frozenset(temp_state.x), frozenset(state.o)) not in self.dict_moves.keys():\n",
    "                self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0\n",
    " \n",
    "        else:\n",
    "        \n",
    "        \n",
    "            keys= [k for k in self.dict_moves.keys() if (k[1] == state.o and k[0]==state.x )]\n",
    "\n",
    "            if keys:\n",
    "                best_move = None\n",
    "                max_value = float('-inf')\n",
    "\n",
    "                for move in available_moves:\n",
    "                    temp_state=deepcopy(state)\n",
    "                    temp_state.x.add(move)\n",
    "                    hashable_state = (frozenset(temp_state.x), frozenset(temp_state.o))\n",
    "\n",
    "                    if hashable_state in self.dict_moves.keys():\n",
    "                        move_value = self.dict_moves[hashable_state]\n",
    "                        if move_value > max_value:\n",
    "                            max_value = move_value\n",
    "                            best_move = move\n",
    "\n",
    "                \n",
    "                if best_move is None:\n",
    "                    \n",
    "                    best_move = choice(list(available_moves))\n",
    "                    #state.x.add(best_move)\n",
    "\n",
    "                    temp_state.x.add(best_move)\n",
    "                    self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0\n",
    "            else:\n",
    "                best_move = choice(list(available_moves))\n",
    "                #state.x.add(best_move)\n",
    "                temp_state.x.add(best_move)\n",
    "                self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0\n",
    "\n",
    "\n",
    "        \"\"\" if state_value(temp_state) == 1:\n",
    "            keys= [k for k in self.dict_moves.keys() if (k[1] == temp_state.o and k[0]==temp_state.x )]\n",
    "            new_state = sorted(keys, key=lambda k: self.dict_moves[k], reverse=True)[0]\n",
    "            self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0.99*self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]+0.01*(0.5+0.01*self.dict_moves[(frozenset(new_state[0]), frozenset(new_state[1]))])\n",
    "        elif state_value(temp_state) == -1:\n",
    "            keys= [k for k in self.dict_moves.keys() if (k[1] == temp_state.o and k[0]==temp_state.x )]\n",
    "            new_state = sorted(keys, key=lambda k: self.dict_moves[k], reverse=True)[0]\n",
    "    \n",
    "            self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0.99*self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]+0.01*(-0.5+0.01*self.dict_moves[(frozenset(new_state.x), frozenset(new_state.o))])\n",
    "        \"\"\"\n",
    "        \n",
    "        keys= [k for k in self.dict_moves.keys() if (k[1] == temp_state.o and k[0]==temp_state.x )]\n",
    "      \n",
    "    \n",
    "        if keys:\n",
    "            new_state = sorted(keys, key=lambda k: self.dict_moves[k], reverse=True)[0]\n",
    "\n",
    "            self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0.99*self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]+0.01*(1+0.01*self.dict_moves[(frozenset(new_state[0]), frozenset(new_state[1]))])\n",
    "        \n",
    "        return best_move\n",
    "    \n",
    "    def add_winning(self):\n",
    "        self._winning_games+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     \\nvalue_dictionary=defaultdict(float)\\nepsilon=0.001\\n\\n\\n#o\\np1=RandomPlayer(-1)\\n#x\\np2=Agent(1)\\n\\nnum_iterations=500_000\\n\\nfor steps in tqdm(range(num_iterations)):\\n    trajectory=random_game_2(p1,p2)\\n   \\n    # i compute the final reward\\n    final_reward=state_value(trajectory[-1])\\n\\n    #update all the state according to this reward\\n    for s in trajectory:\\n        hashable_state=(frozenset(s.x),frozenset(s.o))\\n        p2.dict_moves[hashable_state]= p2.dict_moves[hashable_state]+epsilon*(final_reward-p2.dict_moves[hashable_state])\\n\\n\\nprint(\"Winning percentage of the agent \",p2._winning_games/num_iterations*100) '"
      ]
     },
     "execution_count": 1463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self._symbol=symbol\n",
    "        self._dict_moves=defaultdict(float)\n",
    "        self._winning_games=0\n",
    "\n",
    "    @property\n",
    "    def symbol(self)-> int:\n",
    "        return self._symbol\n",
    "    \n",
    "    @property\n",
    "    def dict_moves(self)-> int:\n",
    "        return self._dict_moves\n",
    "    \n",
    "    def move(self,available_moves, state)->int:\n",
    "\n",
    "        if random()<0.05:\n",
    "            best_move = choice(list(available_moves))\n",
    "            temp_state = deepcopy(state) \n",
    "            temp_state.x.add(best_move)\n",
    "            if (frozenset(state.x), frozenset(state.o)) not in self.dict_moves.keys():\n",
    "                self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0.001\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #keys= [k for k in self.dict_moves.keys() if (k[1] == state.o and len(k[0])==len(state.x)+1 )]\n",
    "            keys= [k for k in self.dict_moves.keys() if (k[1] == state.o and k[0]==state.x )]\n",
    "\n",
    "            if keys:\n",
    "\n",
    "                best_move = None\n",
    "                max_value = float('-inf')\n",
    "\n",
    "                for move in available_moves:\n",
    "                    temp_state = deepcopy(state)  \n",
    "                    temp_state.x.add(move)\n",
    "                    hashable_state = (frozenset(temp_state.x), frozenset(temp_state.o))\n",
    "\n",
    "                    if hashable_state in self.dict_moves.keys():\n",
    "                        move_value = self.dict_moves[hashable_state]\n",
    "                        if move_value > max_value:\n",
    "                            max_value = move_value\n",
    "                            best_move = move\n",
    "\n",
    "                if best_move is None:\n",
    "                    best_move = choice(list(available_moves))\n",
    "                    #state.x.add(best_move)\n",
    "                    temp_state = deepcopy(state) \n",
    "                    temp_state.x.add(best_move)\n",
    "                    self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0.001\n",
    "            else:\n",
    "                best_move = choice(list(available_moves))\n",
    "                #state.x.add(best_move)\n",
    "                temp_state = deepcopy(state) \n",
    "                temp_state.x.add(best_move)\n",
    "                self.dict_moves[(frozenset(temp_state.x), frozenset(temp_state.o))]=0.001\n",
    "                \n",
    "        return best_move\n",
    "    \n",
    "    def add_winning(self):\n",
    "        self._winning_games+=1\n",
    "\n",
    "def random_game_2(p1,p2):\n",
    "    \n",
    "    trajectory=list()\n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "\n",
    "\n",
    "    players=[p1,p2]\n",
    "    index=choice([0,1])\n",
    "    #index=1\n",
    "    #index=0\n",
    "    while True:\n",
    "        \n",
    "        current_player=players[index]\n",
    "\n",
    "        move=current_player.move(list(available),state)\n",
    "\n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(move) \n",
    "            if(win(state.o)) or not available:\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(move) \n",
    "            if(win(state.x)) or not available:\n",
    "                current_player.add_winning()\n",
    "                break\n",
    "    \n",
    "        index=1-index\n",
    "\n",
    "    return trajectory\n",
    "\"\"\"     \n",
    "value_dictionary=defaultdict(float)\n",
    "epsilon=0.001\n",
    "\n",
    "\n",
    "#o\n",
    "p1=RandomPlayer(-1)\n",
    "#x\n",
    "p2=Agent(1)\n",
    "\n",
    "num_iterations=500_000\n",
    "\n",
    "for steps in tqdm(range(num_iterations)):\n",
    "    trajectory=random_game_2(p1,p2)\n",
    "   \n",
    "    # i compute the final reward\n",
    "    final_reward=state_value(trajectory[-1])\n",
    "\n",
    "    #update all the state according to this reward\n",
    "    for s in trajectory:\n",
    "        hashable_state=(frozenset(s.x),frozenset(s.o))\n",
    "        p2.dict_moves[hashable_state]= p2.dict_moves[hashable_state]+epsilon*(final_reward-p2.dict_moves[hashable_state])\n",
    "\n",
    "\n",
    "print(\"Winning percentage of the agent \",p2._winning_games/num_iterations*100) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
