{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: 25/12 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: 06/01 ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see it this way: picking 3 numbers whose sum is 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laura\\Desktop\\Università\\Polito\\Computational_Intelligence\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice, random, randint\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(sum(c)==15 for c in combinations({1,2,3,4,5,6},3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "State= namedtuple('State',['x','o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC=[2,7,6,\n",
    "       9,5,1,\n",
    "       4,3,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\" Checks if elements is winning\"\"\"\n",
    "    return any(sum(c)==15 for c in combinations(elements,3))\n",
    "\n",
    "def state_value(pos:State):\n",
    "    \"\"\"Evaluate position: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\" Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            index=r*3+c\n",
    "            if MAGIC[index] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[index] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('-', end='')\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self._symbol=symbol\n",
    "        \n",
    "    @property\n",
    "    def symbol(self)-> int:\n",
    "        return self._symbol\n",
    "    \n",
    "    def move(self,available_moves, state=None):\n",
    "        #print(\"random move\")\n",
    "        return choice(list(available_moves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, symbol):\n",
    "        self.q_table = {}\n",
    "        self.symbol=symbol\n",
    "        self._winning_games=0\n",
    "        self._drawn_games=0\n",
    "        self.exploration_rate=0.5\n",
    "        self.alpha=0.7\n",
    "        self.gamma=0.9\n",
    "        self.is_train=True\n",
    "\n",
    "\n",
    "    def move(self, available_moves, state)-> int:\n",
    "        \n",
    "        if self.exploration_rate>0.01:\n",
    "            self.exploration_rate*=0.99\n",
    "\n",
    "        if self.alpha>0.1:\n",
    "            self.alpha*=0.99\n",
    "\n",
    "        if self.gamma<0.99:\n",
    "            self.gamma*=1.01\n",
    "\n",
    "        state_key = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        if random() < self.exploration_rate:\n",
    "            # Scelta casuale di un'azione\n",
    "            action = choice(list(available_moves))\n",
    "        else:\n",
    "            # Scelta dell'azione basata sulla Q-table\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys(range(1,10), 0)\n",
    "                \n",
    "            action = max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.update_q_table(action, available_moves, state)\n",
    "       \n",
    "        if action not in available_moves:\n",
    "            action=choice(list(available_moves))\n",
    "            #self.update_q_table(action, available_moves, state)\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys([action], 0)\n",
    "            return action\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def add_winning(self):\n",
    "        self._winning_games+=1\n",
    "        \n",
    "    def add_drawn(self):\n",
    "        self._drawn_games+=1\n",
    "\n",
    "    def update_q_table(self, action, available_moves, state, reward=None):\n",
    "        #print(\"updating\")\n",
    "        state_key=(frozenset(state.x), frozenset(state.o))\n",
    "        if reward is None:\n",
    "\n",
    "            if action not in available_moves:\n",
    "                reward=-5\n",
    "                \n",
    "            else:\n",
    "                reward=0.1\n",
    "                \n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = dict.fromkeys([action], 0)\n",
    "\n",
    "        new_state = deepcopy(state)\n",
    "        new_state.x.add(action)\n",
    "        next_state_key = (frozenset(new_state.x), frozenset(new_state.o))\n",
    "\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = dict.fromkeys(range(1,10), 0)\n",
    "\n",
    "        self.q_table[state_key][action] = (1 - self.alpha) * self.q_table[state_key].get(action, 0) + self.alpha * (reward + self.gamma * (max(self.q_table[next_state_key].values(), default=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def game(p1,p2, index):\n",
    "    \n",
    "    trajectory=list()\n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "    \n",
    "    players=[p1,p2]\n",
    "\n",
    "    while True:\n",
    "    \n",
    "        current_player=players[index]\n",
    "\n",
    "        move=current_player.move(list(available),state)\n",
    "        trajectory.append((deepcopy(state), move))\n",
    "        available.remove(move)\n",
    "        \n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            if(win(state.o)) or not available:\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            if(win(state.x)) or not available:\n",
    "                break\n",
    "        last_index=index\n",
    "        index=1-index\n",
    "\n",
    "    # i compute the final reward\n",
    "        \n",
    "    final_reward=state_value(state)\n",
    "    #print(index, final_reward)\n",
    "    return trajectory, final_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o\n",
    "p1=RandomPlayer(-1)\n",
    "#x\n",
    "p2=QLearningAgent(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 612/100000 [00:00<02:10, 759.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:29<00:00, 1119.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q AGENT STARTS\n",
      "Winning percentage of the agent  92.252\n",
      "Drawn percentage of the agent  6.582000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:39<00:00, 1005.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM STARTS\n",
      "Winning percentage of the agent  59.297\n",
      "Drawn percentage of the agent  27.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(p1,p2, index):\n",
    "\n",
    "\n",
    "      num_iterations=100_000\n",
    "      p2._winning_games=0\n",
    "      p2._drawn_games=0\n",
    "\n",
    "      for _ in tqdm(range(num_iterations)):\n",
    "\n",
    "            trajectory, final_reward=game(p1,p2,index)\n",
    "\n",
    "            if final_reward ==0:\n",
    "                  p2.add_drawn()\n",
    "            elif final_reward==1:\n",
    "                  p2.add_winning()\n",
    "\n",
    "            if final_reward == -1:\n",
    "            \n",
    "                  s=trajectory[-2][0]\n",
    "                  a=trajectory[-2][1]\n",
    "\n",
    "            else:  \n",
    "                  s=trajectory[-1][0]\n",
    "                  a=trajectory[-1][1]   \n",
    "                  \n",
    "\n",
    "            p2.update_q_table(a,(),s,final_reward)\n",
    "            \n",
    "\n",
    "      if index==0:\n",
    "            print(\"RANDOM STARTS\")\n",
    "      else:\n",
    "            print(\"Q AGENT STARTS\")\n",
    "      print(\"Winning percentage of the agent \",p2._winning_games/num_iterations*100)\n",
    "      print(\"Drawn percentage of the agent \",p2._drawn_games/num_iterations*100)\n",
    "      #for c,v in p2.q_table.items():\n",
    "            #print(c,v)\n",
    "      \n",
    "train(p1,p2,1)\n",
    "train(p1,p2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 602/5000 [00:00<00:00, 6007.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 10390.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM STARTS\n",
      "winning or tie 89.72 %\n",
      "losing  10.280000000000001 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 16913.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q AGENT STARTS\n",
      "winning or tie 99.48 %\n",
      "losing  0.52 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test(p1,p2, index):\n",
    "    \n",
    "    global count_winning\n",
    "    global count_losing\n",
    "    global count_tie\n",
    "\n",
    "    #trajectory=list()\n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "    \n",
    "    players=[p1,p2]\n",
    "    \n",
    "\n",
    "    while True:\n",
    "    \n",
    "        current_player=players[index]\n",
    "\n",
    "        move=current_player.move(list(available),state)\n",
    "        #trajectory.append((deepcopy(state), move))\n",
    "        \n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            available.remove(move) \n",
    "            if(win(state.o)) or not available:\n",
    "                if (win(state.o)):\n",
    "                    #print(\"O \")\n",
    "                    \n",
    "                    count_losing+=1\n",
    "                else: \n",
    "                    #print(\"tie\")\n",
    "                    count_tie+=1\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            #trajectory.append((deepcopy(state),move))\n",
    "            available.remove(move) \n",
    "            if(win(state.x)) or not available:\n",
    "                #current_player.add_winning()\n",
    "                if(win(state.x)):\n",
    "                    #print(\"X\")\n",
    "                    count_winning+=1\n",
    "                else:\n",
    "                    #print(\"tie\")\n",
    "                    count_tie+=1\n",
    "\n",
    "                break\n",
    "                \n",
    "        index=1-index\n",
    "    return\n",
    "\n",
    "\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "num_iterations=5000\n",
    "p2.is_train=False\n",
    "\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    test(p1,p2,0)\n",
    "\n",
    "print(\"RANDOM STARTS\")\n",
    "    \n",
    "print(\"winning or tie\", (count_winning+count_tie)/num_iterations*100,\"%\")\n",
    "print(\"losing \", count_losing/num_iterations*100,\"%\")\n",
    "\n",
    "\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    test(p1,p2,1)\n",
    "    \n",
    "print(\"Q AGENT STARTS\")\n",
    "\n",
    "print(\"winning or tie\", (count_winning+count_tie)/num_iterations*100,\"%\")\n",
    "print(\"losing \", count_losing/num_iterations*100,\"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "for c,v in p2.q_table.items():\n",
    "    print(c,v) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTECARLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MontecarloAgent:\n",
    "    def __init__(self, symbol):\n",
    "        self.q_table = {}\n",
    "        self.symbol=symbol\n",
    "        self._winning_games=0\n",
    "        self._drawn_games=0\n",
    "        self.exploration_rate=0.1\n",
    "        self.rewards=[]\n",
    "        self.gamma=0.9\n",
    "        self.is_train=True\n",
    "\n",
    "\n",
    "    def move(self, available_moves, state)-> int:\n",
    "        \n",
    "        if self.exploration_rate>0.01:\n",
    "            self.exploration_rate*=0.99\n",
    "\n",
    "\n",
    "        if self.gamma<0.99:\n",
    "            self.gamma*=1.01\n",
    "\n",
    "        state_key = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        if random() < self.exploration_rate:\n",
    "            # Scelta casuale di un'azione\n",
    "            action = choice(list(available_moves))\n",
    "        else:\n",
    "            # Scelta dell'azione basata sulla Q-table\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys(range(1,10), 0)\n",
    "                \n",
    "            action = max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
    "\n",
    "\n",
    "        if action not in available_moves:\n",
    "            reward=-5\n",
    "            \n",
    "        else:\n",
    "            reward=0.1\n",
    "            \n",
    "        if action not in available_moves:\n",
    "            action=choice(list(available_moves))\n",
    "            #self.update_q_table(action, available_moves, state)\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys([action], 0)\n",
    "            return action\n",
    "            \n",
    "        self.rewards.append(reward)\n",
    "        return action\n",
    "\n",
    "    def add_winning(self):\n",
    "        self._winning_games+=1\n",
    "        \n",
    "    def add_drawn(self):\n",
    "        self._drawn_games+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------TRAIN----------------\n",
      "RANDOM STARTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:33<00:00, 1492.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning percentage of the agent  79.706\n",
      "Tie percentage  5.726\n",
      "win and ties 85.432\n",
      "MONTECARLO AGENT STARTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:32<00:00, 1523.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning percentage of the agent  87.862\n",
      "Tie percentage  9.478\n",
      "win and ties 97.34\n",
      "--------------TEST----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 12395.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM STARTS\n",
      "winning or tie 87.29 %\n",
      "losing  12.709999999999999 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 16541.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTECARLO AGENT STARTS\n",
      "winning or tie 97.42 %\n",
      "losing  2.58 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_montecarlo(p1,p2, index):\n",
    "\n",
    "    num_iterations=50_000\n",
    "    p2._winning_games=0\n",
    "    p2._drawn_games=0\n",
    "\n",
    "    for _ in tqdm(range(num_iterations)):\n",
    "\n",
    "        p2.rewards=[]\n",
    "        trajectory, final_reward=game(p1,p2,index)\n",
    "\n",
    "        if final_reward ==0:\n",
    "                p2.add_drawn()\n",
    "        elif final_reward==1:\n",
    "                p2.add_winning()\n",
    "\n",
    "\n",
    "        #p2.rewards.append(final_reward)\n",
    "     \n",
    "        for state,action in trajectory:\n",
    "            if (frozenset(state.x), frozenset(state.o)) in p2.q_table:\n",
    "                p2.q_table[(frozenset(state.x), frozenset(state.o))][action]+=0.001 * (final_reward -  p2.q_table[(frozenset(state.x), frozenset(state.o))][action])\n",
    "                \n",
    "    print(\"Winning percentage of the agent \",p2._winning_games/num_iterations*100) \n",
    "    print(\"Tie percentage \",p2._drawn_games/num_iterations*100 )\n",
    "    print(\"win and ties\",(p2._winning_games+p2._drawn_games)/num_iterations*100 )\n",
    "\n",
    "            \n",
    "def test_montecarlo(p1,p2, index):\n",
    "    \n",
    "    global count_winning\n",
    "    global count_losing\n",
    "    global count_tie\n",
    "\n",
    "    #trajectory=list()\n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "    \n",
    "    players=[p1,p2]\n",
    "    \n",
    "\n",
    "    while True:\n",
    "    \n",
    "        current_player=players[index]\n",
    "\n",
    "        move=current_player.move(list(available),state)\n",
    "        #trajectory.append((deepcopy(state), move))\n",
    "        \n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            available.remove(move) \n",
    "            if(win(state.o)) or not available:\n",
    "                if (win(state.o)):\n",
    "                    #print(\"O \")\n",
    "                    \n",
    "                    count_losing+=1\n",
    "                else: \n",
    "                    #print(\"tie\")\n",
    "                    count_tie+=1\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            #trajectory.append((deepcopy(state),move))\n",
    "            available.remove(move) \n",
    "            if(win(state.x)) or not available:\n",
    "                #current_player.add_winning()\n",
    "                if(win(state.x)):\n",
    "                    #print(\"X\")\n",
    "                    count_winning+=1\n",
    "                else:\n",
    "                    #print(\"tie\")\n",
    "                    count_tie+=1\n",
    "\n",
    "                break\n",
    "                \n",
    "        index=1-index\n",
    "    return\n",
    "\n",
    "\n",
    "#o\n",
    "p1=RandomPlayer(-1)\n",
    "#x\n",
    "p2=MontecarloAgent(1)\n",
    "\n",
    "print(\"--------------TRAIN----------------\")\n",
    "\n",
    "print(\"RANDOM STARTS\")\n",
    "train_montecarlo(p1,p2,0)\n",
    "print(\"MONTECARLO AGENT STARTS\")\n",
    "train_montecarlo(p1,p2,1)\n",
    "\n",
    "\n",
    "print(\"--------------TEST----------------\")\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "\n",
    "num_iterations=10000\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    test_montecarlo(p1,p2,0)\n",
    "\n",
    "print(\"RANDOM STARTS\")\n",
    "    \n",
    "print(\"winning or tie\", (count_winning+count_tie)/num_iterations*100,\"%\")\n",
    "print(\"losing \", count_losing/num_iterations*100,\"%\")\n",
    "\n",
    "\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    test_montecarlo(p1,p2,1)\n",
    "\n",
    "print(\"MONTECARLO AGENT STARTS\")\n",
    "\n",
    "print(\"winning or tie\", (count_winning+count_tie)/num_iterations*100,\"%\")\n",
    "print(\"losing \", count_losing/num_iterations*100,\"%\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
