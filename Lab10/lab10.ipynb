{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** Laura Amoroso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: 25/12 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: 06/01 ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the \"Magic Board\" to represent the problem the goal becomes picking 3 numbers whose sum is 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice, random, randint\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is represented by a tuple that contains the positions taken by the 2 players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "State= namedtuple('State',['x','o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC=[2,7,6,\n",
    "       9,5,1,\n",
    "       4,3,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\" Checks if elements is winning\"\"\"\n",
    "    return any(sum(c)==15 for c in combinations(elements,3))\n",
    "\n",
    "def state_value(pos:State):\n",
    "    \"\"\"Evaluate position: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\" Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            index=r*3+c\n",
    "            if MAGIC[index] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[index] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('-', end='')\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a random player, that chooses his moves randomly among the available ones, is defined to play aganist my trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self._symbol=symbol\n",
    "        \n",
    "    @property\n",
    "    def symbol(self)-> int:\n",
    "        return self._symbol\n",
    "    \n",
    "    def move(self,available_moves, state=None):\n",
    "        return choice(list(available_moves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LEARNING"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined an agent that is trained with the Q Learning algorithm. The q table contains the states and the possible moves associated with a value indicating the most promising move. This is represented by an annidated dictionary whose keys are the states and the value is another dictionary that has as keys the actions and as values their qvalues.\n",
    "\n",
    "At each move the agent receives a reward of 0.1 if the action is among the available ones, -5 if it is not, and at the end of the game +1 for a victory, -1 for a lose and 0 for a tie. The formula I used for updating the table is:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "where α is the learning rate, r is the reward obatained by making the action a in the state s, γ is the discount rate  and ![image-2.png](attachment:image-2.png) is the maximum value of the next state.\n",
    "\n",
    "α and γ are tuned during the episodes. WIth a certain probability, the exploration rate, the agent does random moves to explore new states. This value is high during the first 1000 episodes and then decreases as suggested in one of the linked papers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, symbol):\n",
    "        self.q_table = {}\n",
    "        self.symbol=symbol\n",
    "        self._winning_games=0\n",
    "        self._drawn_games=0\n",
    "        self.exploration_rate=0.3\n",
    "        self.alpha=0.7\n",
    "        self.gamma=0.9\n",
    "        self.is_train=True\n",
    "\n",
    "\n",
    "    def move(self, available_moves, state)-> int:\n",
    "        \"\"\"    \n",
    "        if self.exploration_rate>0.01:\n",
    "            self.exploration_rate*=0.99\n",
    "        \"\"\"\n",
    "        if self.alpha>0.1:\n",
    "            self.alpha*=0.99\n",
    "\n",
    "        if self.gamma<0.99:\n",
    "            self.gamma*=1.01\n",
    "\n",
    "        state_key = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        if random() < self.exploration_rate:\n",
    "          \n",
    "            action = choice(list(available_moves))\n",
    "        else:\n",
    "\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys(range(1,10), 0)\n",
    "                \n",
    "            action = max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.update_q_table(action, available_moves, state)\n",
    "       \n",
    "        if action not in available_moves:\n",
    "            # if the move wasn't available choose a random one\n",
    "            action=choice(list(available_moves))\n",
    "\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys([action], 0)\n",
    "            return action\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def add_winning(self):\n",
    "        self._winning_games+=1\n",
    "        \n",
    "    def add_drawn(self):\n",
    "        self._drawn_games+=1\n",
    "\n",
    "    def update_q_table(self, action, available_moves, state, reward=None):\n",
    "        #print(\"updating\")\n",
    "        state_key=(frozenset(state.x), frozenset(state.o))\n",
    "        if reward is None:\n",
    "\n",
    "            if action not in available_moves:\n",
    "                reward=-5\n",
    "                \n",
    "            else:\n",
    "                reward=0.1\n",
    "                \n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = dict.fromkeys([action], 0)\n",
    "\n",
    "        new_state = deepcopy(state)\n",
    "        new_state.x.add(action)\n",
    "        next_state_key = (frozenset(new_state.x), frozenset(new_state.o))\n",
    "\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = dict.fromkeys(range(1,10), 0)\n",
    "\n",
    "        self.q_table[state_key][action] = (1 - self.alpha) * self.q_table[state_key].get(action, 0) + self.alpha * (reward + self.gamma * (max(self.q_table[next_state_key].values(), default=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def game(p1,p2, index):\n",
    "    \n",
    "    trajectory=list()\n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "    \n",
    "    players=[p1,p2]\n",
    "\n",
    "    while True:\n",
    "    \n",
    "        current_player=players[index]\n",
    "\n",
    "        move=current_player.move(list(available),state)\n",
    "        trajectory.append((deepcopy(state), move))\n",
    "        available.remove(move)\n",
    "        \n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            if(win(state.o)) or not available:\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            if(win(state.x)) or not available:\n",
    "                break\n",
    "        last_index=index\n",
    "        index=1-index\n",
    "\n",
    "    # i compute the final reward\n",
    "        \n",
    "    final_reward=state_value(state)\n",
    "\n",
    "    return trajectory, final_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent always plays with the X symbol and I train it by doing 100_000 iterations in which he starts the game and other 100_000 in which he plays as second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o\n",
    "p1=RandomPlayer(-1)\n",
    "#x\n",
    "p2=QLearningAgent(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 878/100000 [00:00<01:41, 975.19it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:14<00:00, 1339.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q AGENT STARTS\n",
      "Winning percentage of the agent  79.85900000000001\n",
      "Drawn percentage of the agent  16.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:09<00:00, 1445.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM STARTS\n",
      "Winning percentage of the agent  52.054\n",
      "Drawn percentage of the agent  32.072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(p1,p2, index):\n",
    "\n",
    "\n",
    "      num_iterations=100_000\n",
    "      p2._winning_games=0\n",
    "      p2._drawn_games=0\n",
    "\n",
    "      for episodes in tqdm(range(num_iterations)):\n",
    "\n",
    "            # I used the update of the exploration rate suggested in the paper:\n",
    "            # https://www.researchgate.net/publication/369096697_Reinforcement_Learning_Playing_Tic-Tac-Toe\n",
    "            \n",
    "            if episodes>1000 and p2.exploration_rate>0.01:\n",
    "                  p2.exploration_rate=((1-p2.exploration_rate)*10)/episodes\n",
    "\n",
    "                  \n",
    "            trajectory, final_reward=game(p1,p2,index)\n",
    "\n",
    "            if final_reward ==0:\n",
    "                  p2.add_drawn()\n",
    "            elif final_reward==1:\n",
    "                  p2.add_winning()\n",
    "\n",
    "            if final_reward == -1:\n",
    "            \n",
    "                  s=trajectory[-2][0]\n",
    "                  a=trajectory[-2][1]\n",
    "\n",
    "            else:  \n",
    "                  s=trajectory[-1][0]\n",
    "                  a=trajectory[-1][1]   \n",
    "                  \n",
    "\n",
    "            p2.update_q_table(a,(),s,final_reward)\n",
    "            \n",
    "\n",
    "      if index==0:\n",
    "            print(\"RANDOM STARTS\")\n",
    "      else:\n",
    "            print(\"Q AGENT STARTS\")\n",
    "      print(\"Winning percentage of the agent \",p2._winning_games/num_iterations*100)\n",
    "      print(\"Drawn percentage of the agent \",p2._drawn_games/num_iterations*100)\n",
    "\n",
    "train(p1,p2,1)\n",
    "train(p1,p2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1523/5000 [00:00<00:00, 15086.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 13551.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM STARTS\n",
      "winning or tie 83.89999999999999 %\n",
      "losing  16.1 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 14367.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q AGENT STARTS\n",
      "winning or tie 97.2 %\n",
      "losing  2.8000000000000003 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test(p1,p2, index):\n",
    "    \n",
    "    global count_winning\n",
    "    global count_losing\n",
    "    global count_tie\n",
    "    \n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "    \n",
    "    players=[p1,p2]\n",
    "    \n",
    "\n",
    "    while True:\n",
    "    \n",
    "        current_player=players[index]\n",
    "        move=current_player.move(list(available),state)\n",
    "      \n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            available.remove(move) \n",
    "            if(win(state.o)) or not available:\n",
    "                if (win(state.o)):\n",
    "                    count_losing+=1\n",
    "                else: \n",
    "                    count_tie+=1\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            available.remove(move) \n",
    "            if(win(state.x)) or not available:\n",
    "                if(win(state.x)):\n",
    "                    count_winning+=1\n",
    "                else:\n",
    "                    count_tie+=1\n",
    "\n",
    "                break\n",
    "                \n",
    "        index=1-index\n",
    "    return\n",
    "\n",
    "\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "num_iterations=5000\n",
    "\n",
    "# when not in train mode the Q table is not updated\n",
    "p2.is_train=False\n",
    "\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    test(p1,p2,0)\n",
    "\n",
    "print(\"RANDOM STARTS\")\n",
    "    \n",
    "print(\"winning or tie\", (count_winning+count_tie)/num_iterations*100,\"%\")\n",
    "print(\"losing \", count_losing/num_iterations*100,\"%\")\n",
    "\n",
    "\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    test(p1,p2,1)\n",
    "    \n",
    "print(\"Q AGENT STARTS\")\n",
    "\n",
    "print(\"winning or tie\", (count_winning+count_tie)/num_iterations*100,\"%\")\n",
    "print(\"losing \", count_losing/num_iterations*100,\"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "for c,v in p2.q_table.items():\n",
    "    print(c,v) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTECARLO "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the montecarlo agent I chose the same design but I propagate the reward only at the end of the game to all the state visited during the game. This is the formula I use for updating\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "where α is the learning rate and G is the final reward obtained at the end of the game.\n",
    "\n",
    "I tried to use the sum of the move rewards obtained during the whole game instead of just the final one, but it gave me worse performances. Here I use a reward of 0.1 for the acceptable moves and -0.1 for the unacceptable ones. I also use a different technique to update the exploration rate, because the previous one does not work well since I do less iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MontecarloAgent:\n",
    "    def __init__(self, symbol):\n",
    "        self.q_table = {}\n",
    "        self.symbol=symbol\n",
    "        self._winning_games=0\n",
    "        self._drawn_games=0\n",
    "        self.exploration_rate=0.1\n",
    "        self.rewards=[]\n",
    "        self.gamma=0.9\n",
    "        self.is_train=True\n",
    "\n",
    "\n",
    "    def move(self, available_moves, state)-> int:\n",
    "        \n",
    "        if self.exploration_rate>0.01:\n",
    "            self.exploration_rate*=0.99\n",
    "        \n",
    "\n",
    "        if self.gamma<0.99:\n",
    "            self.gamma*=1.01\n",
    "\n",
    "        state_key = (frozenset(state.x), frozenset(state.o))\n",
    "\n",
    "        if random() < self.exploration_rate:\n",
    "            # sometimes make random moves\n",
    "            action = choice(list(available_moves))\n",
    "        else:\n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys(range(1,10), 0)\n",
    "            \n",
    "            #choose the action based on the q table   \n",
    "            action = max(self.q_table[state_key], key=self.q_table[state_key].get)\n",
    "\n",
    "\n",
    "        if action not in available_moves:\n",
    "            reward=-0.1\n",
    "            \n",
    "        else:\n",
    "            reward=0.1\n",
    "            \n",
    "        self.rewards.append(reward)\n",
    "        if action not in available_moves:\n",
    "            action=choice(list(available_moves))\n",
    "            \n",
    "            if state_key not in self.q_table:\n",
    "                self.q_table[state_key] = dict.fromkeys([action], 0)\n",
    "            return action\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def add_winning(self):\n",
    "        self._winning_games+=1\n",
    "        \n",
    "    def add_drawn(self):\n",
    "        self._drawn_games+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------TRAIN----------------\n",
      "RANDOM STARTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:43<00:00, 2320.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning percentage of the agent  78.682\n",
      "Tie percentage  7.154000000000001\n",
      "win and ties 85.836\n",
      "MONTECARLO AGENT STARTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:47<00:00, 2087.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning percentage of the agent  87.258\n",
      "Tie percentage  6.031000000000001\n",
      "win and ties 93.289\n",
      "--------------TEST----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 15849.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM STARTS\n",
      "winning or tie 86.63 %\n",
      "losing  13.370000000000001 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 15624.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTECARLO AGENT STARTS\n",
      "winning or tie 93.46 %\n",
      "losing  6.54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_montecarlo(p1,p2, index):\n",
    "\n",
    "    num_iterations=100_000\n",
    "    p2._winning_games=0\n",
    "    p2._drawn_games=0\n",
    "\n",
    "    for episodes in tqdm(range(num_iterations)):\n",
    "\n",
    "        #p2.rewards=[]\n",
    "        trajectory, final_reward=game(p1,p2,index)\n",
    "\n",
    "        if final_reward ==0:\n",
    "                p2.add_drawn()\n",
    "        elif final_reward==1:\n",
    "                p2.add_winning()\n",
    "\n",
    "        for state,action in trajectory:\n",
    "            if (frozenset(state.x), frozenset(state.o)) in p2.q_table:\n",
    "                p2.q_table[(frozenset(state.x), frozenset(state.o))][action]+=0.001 * (final_reward -  p2.q_table[(frozenset(state.x), frozenset(state.o))][action])\n",
    "                \n",
    "    print(\"Winning percentage of the agent \",p2._winning_games/num_iterations*100) \n",
    "    print(\"Tie percentage \",p2._drawn_games/num_iterations*100 )\n",
    "    print(\"win and ties\",(p2._winning_games+p2._drawn_games)/num_iterations*100 )\n",
    "\n",
    "            \n",
    "def test_montecarlo(p1,p2, index):\n",
    "    \n",
    "    global count_winning\n",
    "    global count_losing\n",
    "    global count_tie\n",
    "\n",
    "    state=State(set(), set())\n",
    "    available=set(range(1,9+1))\n",
    "    \n",
    "    players=[p1,p2]\n",
    "    \n",
    "\n",
    "    while True:\n",
    "    \n",
    "        current_player=players[index]\n",
    "\n",
    "        move=current_player.move(list(available),state)\n",
    "    \n",
    "        if(current_player.symbol == -1):\n",
    "            state.o.add(move)\n",
    "            available.remove(move) \n",
    "            if(win(state.o)) or not available:\n",
    "                if (win(state.o)):\n",
    "                    count_losing+=1\n",
    "                else: \n",
    "                    count_tie+=1\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            state.x.add(move)\n",
    "            available.remove(move) \n",
    "            if(win(state.x)) or not available:\n",
    "                if(win(state.x)):\n",
    "                    count_winning+=1\n",
    "                else:\n",
    "                    count_tie+=1\n",
    "\n",
    "                break\n",
    "                \n",
    "        index=1-index\n",
    "    return\n",
    "\n",
    "\n",
    "#o\n",
    "p1=RandomPlayer(-1)\n",
    "#x\n",
    "p2=MontecarloAgent(1)\n",
    "\n",
    "print(\"--------------TRAIN----------------\")\n",
    "\n",
    "print(\"RANDOM STARTS\")\n",
    "train_montecarlo(p1,p2,0)\n",
    "print(\"MONTECARLO AGENT STARTS\")\n",
    "train_montecarlo(p1,p2,1)\n",
    "\n",
    "\n",
    "print(\"--------------TEST----------------\")\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "\n",
    "num_iterations=10000\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    test_montecarlo(p1,p2,0)\n",
    "\n",
    "print(\"RANDOM STARTS\")\n",
    "    \n",
    "print(\"winning or tie\", (count_winning+count_tie)/num_iterations*100,\"%\")\n",
    "print(\"losing \", count_losing/num_iterations*100,\"%\")\n",
    "\n",
    "\n",
    "count_winning=0\n",
    "count_losing=0\n",
    "count_tie=0\n",
    "\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    test_montecarlo(p1,p2,1)\n",
    "\n",
    "print(\"MONTECARLO AGENT STARTS\")\n",
    "\n",
    "print(\"winning or tie\", (count_winning+count_tie)/num_iterations*100,\"%\")\n",
    "print(\"losing \", count_losing/num_iterations*100,\"%\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
